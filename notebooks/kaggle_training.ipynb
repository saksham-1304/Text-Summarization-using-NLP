{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f841629",
   "metadata": {},
   "source": [
    "# ğŸ“ Text Summarization â€” BART Fine-tuning on SAMSum\n",
    "\n",
    "This notebook trains **facebook/bart-large-cnn** on the **SAMSum** dialogue summarization dataset.\n",
    "\n",
    "**Requirements:**\n",
    "- Kaggle: Enable **GPU T4 x2** under Settings â†’ Accelerator\n",
    "- Colab: Runtime â†’ Change runtime type â†’ **T4 GPU**\n",
    "\n",
    "**Training time:** ~2 hours on T4 GPU (3 epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1ea8ba",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94d0da53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hcanceled\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mTraceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
      "    status = run_func(*args)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
      "    return func(self, options, args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/commands/install.py\", line 423, in run\n",
      "    _, build_failures = build(\n",
      "                        ^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/wheel_builder.py\", line 319, in build\n",
      "    wheel_file = _build_one(\n",
      "                 ^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/wheel_builder.py\", line 193, in _build_one\n",
      "    wheel_path = _build_one_inside_env(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/wheel_builder.py\", line 240, in _build_one_inside_env\n",
      "    wheel_path = build_wheel_legacy(\n",
      "                 ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/operations/build/wheel_legacy.py\", line 83, in build_wheel_legacy\n",
      "    output = call_subprocess(\n",
      "             ^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/utils/subprocess.py\", line 151, in call_subprocess\n",
      "    line: str = proc.stdout.readline()\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen codecs>\", line 319, in decode\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/pip3\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
      "    return command.main(cmd_args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
      "    return self._main(args)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
      "    return run(options, args)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 216, in exc_logging_wrapper\n",
      "    logger.debug(\"Exception information:\", exc_info=True)\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1527, in debug\n",
      "    self._log(DEBUG, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1684, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1700, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1762, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1028, in handle\n",
      "    self.emit(record)\n",
      "  File \"/usr/lib/python3.12/logging/handlers.py\", line 75, in emit\n",
      "    logging.FileHandler.emit(self, record)\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1280, in emit\n",
      "    StreamHandler.emit(self, record)\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1160, in emit\n",
      "    msg = self.format(record)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 999, in format\n",
      "    return fmt.format(record)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/utils/logging.py\", line 112, in format\n",
      "    formatted = super().format(record)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 711, in format\n",
      "    record.exc_text = self.formatException(record.exc_info)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 661, in formatException\n",
      "    traceback.print_exception(ei[0], ei[1], tb, None, sio)\n",
      "  File \"/usr/lib/python3.12/traceback.py\", line 124, in print_exception\n",
      "    te = TracebackException(type(value), value, tb, limit=limit, compact=True)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/traceback.py\", line 733, in __init__\n",
      "    self.stack = StackSummary._extract_from_extended_frame_gen(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/traceback.py\", line 438, in _extract_from_extended_frame_gen\n",
      "    f.line\n",
      "  File \"/usr/lib/python3.12/traceback.py\", line 323, in line\n",
      "    self._line = linecache.getline(self.filename, self.lineno)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/linecache.py\", line 30, in getline\n",
      "    lines = getlines(filename, module_globals)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/linecache.py\", line 46, in getlines\n",
      "    return updatecache(filename, module_globals)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/linecache.py\", line 141, in updatecache\n",
      "    with tokenize.open(fullname) as fp:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/tokenize.py\", line 459, in open\n",
      "    encoding, lines = detect_encoding(buffer.readline)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/tokenize.py\", line 428, in detect_encoding\n",
      "    first = read_or_stop()\n",
      "            ^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/tokenize.py\", line 386, in read_or_stop\n",
      "    return readline()\n",
      "           ^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers==4.36.2 datasets==2.16.1 evaluate==0.4.1 \\\n",
    "    rouge-score==0.1.2 accelerate==0.25.0 sentencepiece protobuf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20585a2f",
   "metadata": {},
   "source": [
    "## 2. Verify GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3425f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0+cu128\n",
      "CUDA available: True\n",
      "GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "   \n",
    "else:\n",
    "    print(\"âš ï¸ No GPU detected! Training will be very slow. Enable GPU in notebook settings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8899acf",
   "metadata": {},
   "source": [
    "## 3. Load SAMSum Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca8beac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"samsum\")\n",
    "print(f\"Train: {len(dataset['train'])} examples\")\n",
    "print(f\"Validation: {len(dataset['validation'])} examples\")\n",
    "print(f\"Test: {len(dataset['test'])} examples\")\n",
    "print(f\"\\nColumns: {dataset['train'].column_names}\")\n",
    "print(f\"\\n--- Example ---\")\n",
    "print(f\"Dialogue: {dataset['train'][0]['dialogue'][:200]}...\")\n",
    "print(f\"Summary: {dataset['train'][0]['summary']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73073b53",
   "metadata": {},
   "source": [
    "## 4. Load Tokenizer & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9d0e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "MODEL_NAME = \"facebook/bart-large-cnn\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Enable gradient checkpointing for memory efficiency\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "print(f\"Model parameters: {model.num_parameters() / 1e6:.0f}M\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139a9e2c",
   "metadata": {},
   "source": [
    "## 5. Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df344042",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_INPUT_LENGTH = 1024\n",
    "MAX_TARGET_LENGTH = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Tokenize dialogues and summaries.\"\"\"\n",
    "    inputs = examples[\"dialogue\"]\n",
    "    targets = examples[\"summary\"]\n",
    "    \n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    labels = tokenizer(\n",
    "        text_target=targets,\n",
    "        max_length=MAX_TARGET_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    desc=\"Tokenizing\"\n",
    ")\n",
    "\n",
    "print(f\"Tokenized train: {len(tokenized_dataset['train'])}\")\n",
    "print(f\"Tokenized test: {len(tokenized_dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e44c70d",
   "metadata": {},
   "source": [
    "## 6. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c03f3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# Load ROUGE metric\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute ROUGE metrics during evaluation.\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Decode predictions\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in labels (padding)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Compute ROUGE\n",
    "    result = rouge.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True\n",
    "    )\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "\n",
    "OUTPUT_DIR = \"./bart-samsum-finetuned\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    \n",
    "    # Performance\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    # Logging\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "    logging_steps=100,\n",
    "    report_to=\"none\",\n",
    "    \n",
    "    # Generation for eval metrics\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=MAX_TARGET_LENGTH,\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  FP16: {training_args.fp16}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec4a339",
   "metadata": {},
   "source": [
    "## 7. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e97d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Training Complete!\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Total steps: {train_result.metrics['train_steps']}\")\n",
    "print(f\"Training loss: {train_result.metrics['train_loss']:.4f}\")\n",
    "print(f\"Training time: {train_result.metrics['train_runtime']:.0f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab99a63",
   "metadata": {},
   "source": [
    "## 8. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970d55c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating on test set...\")\n",
    "results = trainer.evaluate(tokenized_dataset[\"test\"])\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Test Set Results\")\n",
    "print(f\"{'='*50}\")\n",
    "for key, value in sorted(results.items()):\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4620e13f",
   "metadata": {},
   "source": [
    "## 9. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcee4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create summarization pipeline with fine-tuned model\n",
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=trainer.model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Test dialogues\n",
    "test_dialogues = [\n",
    "    \"\"\"Amanda: Hey, are we meeting today?\n",
    "Jerry: Sure! What time works for you?\n",
    "Amanda: How about 3pm at the coffee shop?\n",
    "Jerry: Perfect, see you there!\n",
    "Amanda: Great, I'll bring the project reports.\"\"\",\n",
    "\n",
    "    \"\"\"Sarah: Did you finish the report?\n",
    "Mike: Almost done, just need to review the numbers.\n",
    "Sarah: The deadline is tomorrow at noon.\n",
    "Mike: I know, I'll send it tonight.\n",
    "Sarah: Great, also include the Q3 projections.\n",
    "Mike: Will do. Should I CC the manager?\n",
    "Sarah: Yes, please CC David.\"\"\",\n",
    "\n",
    "    \"\"\"Tom: Happy birthday! ğŸ‚\n",
    "Lisa: Thank you so much! ğŸ˜Š\n",
    "Tom: Are you doing anything special?\n",
    "Lisa: Just a small dinner with family.\n",
    "Tom: That sounds lovely. Enjoy your day!\"\"\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INFERENCE EXAMPLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, dialogue in enumerate(test_dialogues, 1):\n",
    "    summary = summarizer(dialogue, max_length=128, min_length=10, do_sample=False)\n",
    "    print(f\"\\n--- Example {i} ---\")\n",
    "    print(f\"Dialogue:\\n{dialogue}\")\n",
    "    print(f\"\\nSummary: {summary[0]['summary_text']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6070ee9c",
   "metadata": {},
   "source": [
    "## 10. Save & Download Model\n",
    "\n",
    "Save the fine-tuned model. On Kaggle, you can download from the output tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420f970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "SAVE_DIR = \"./bart-samsum-final\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Save model and tokenizer\n",
    "trainer.model.save_pretrained(SAVE_DIR)\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n",
    "\n",
    "# Show saved files\n",
    "total_size = 0\n",
    "for f in os.listdir(SAVE_DIR):\n",
    "    size = os.path.getsize(os.path.join(SAVE_DIR, f))\n",
    "    total_size += size\n",
    "    print(f\"  {f}: {size / 1e6:.1f} MB\")\n",
    "\n",
    "print(f\"\\nTotal model size: {total_size / 1e9:.2f} GB\")\n",
    "print(f\"\\nModel saved to {SAVE_DIR}\")\n",
    "print(\"\\nğŸ“¥ On Kaggle: Go to Output tab to download the model.\")\n",
    "print(\"ğŸ“¥ On Colab: Use files.download() or mount Google Drive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748478e9",
   "metadata": {},
   "source": [
    "## 11. (Optional) Upload to HuggingFace Hub\n",
    "\n",
    "Push your fine-tuned model to HuggingFace for easy deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f361cd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and fill in to push to HuggingFace Hub\n",
    "\n",
    "# from huggingface_hub import login\n",
    "# login(token=\"hf_YOUR_TOKEN_HERE\")\n",
    "\n",
    "# HF_REPO = \"your-username/bart-samsum-finetuned\"\n",
    "# trainer.model.push_to_hub(HF_REPO)\n",
    "# tokenizer.push_to_hub(HF_REPO)\n",
    "# print(f\"Model pushed to https://huggingface.co/{HF_REPO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621f76a6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## After Training\n",
    "\n",
    "1. **Download** the `bart-samsum-final/` folder\n",
    "2. **Copy** it to your project at `artifacts/model_trainer/bart-samsum-model/`\n",
    "3. **Run** the API server: `python app.py`\n",
    "4. **Open** `http://localhost:8080` for the web UI\n",
    "\n",
    "### Expected Results (3 epochs)\n",
    "\n",
    "| Metric | Score |\n",
    "|--------|-------|\n",
    "| ROUGE-1 | ~0.52 |\n",
    "| ROUGE-2 | ~0.28 |\n",
    "| ROUGE-L | ~0.43 |\n",
    "| ROUGE-Lsum | ~0.48 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
