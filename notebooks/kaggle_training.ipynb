{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f841629",
   "metadata": {},
   "source": [
    "# ðŸ“ Text Summarization â€” BART Fine-tuning on SAMSum\n",
    "\n",
    "This notebook trains **facebook/bart-large-cnn** on the **Samsung SAMSum processed** dataset\n",
    "([koushik7198/Samsung-samsum_processed](https://huggingface.co/datasets/koushik7198/Samsung-samsum_processed)) â€”\n",
    "a **public** dataset, no authentication needed.\n",
    "\n",
    "**Before running:**\n",
    "1. **Enable GPU**: Kaggle â†’ Settings â†’ Accelerator â†’ **GPU T4 x2** (or Colab â†’ Runtime â†’ T4 GPU)\n",
    "\n",
    "**Training time:** ~2 hours on T4 GPU (3 epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1ea8ba",
   "metadata": {},
   "source": [
    "## 1. Clone Repository from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d0da53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "REPO_URL = \"https://github.com/saksham-1304/Text-Summarization-using-NLP.git\"\n",
    "REPO_DIR = \"Text-Summarization-using-NLP\"\n",
    "\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone {REPO_URL}\n",
    "    print(f\"Cloned {REPO_URL}\")\n",
    "else:\n",
    "    !cd {REPO_DIR} && git pull\n",
    "    print(f\"Pulled latest from {REPO_URL}\")\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20585a2f",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3425f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install compatible versions\n",
    "!pip install -q \"transformers>=4.41.0\" \"datasets>=2.16.1\" \"evaluate>=0.4.1\" \\\n",
    "    rouge-score accelerate sentencepiece protobuf \\\n",
    "    huggingface_hub python-box pyyaml pandas pyarrow\n",
    "\n",
    "# â”€â”€ Free pip cache immediately to reclaim ~1-2 GB â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "!pip cache purge -q\n",
    "\n",
    "# Suppress frozen modules debugger warning\n",
    "import os\n",
    "os.environ[\"PYDEVD_DISABLE_FILE_VALIDATION\"] = \"1\"\n",
    "\n",
    "# Show available disk space\n",
    "import shutil\n",
    "total, used, free = shutil.disk_usage(\"/kaggle/working\")\n",
    "print(f\"Dependencies installed. Disk: {free/1e9:.1f} GB free / {total/1e9:.1f} GB total\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8899acf",
   "metadata": {},
   "source": [
    "## 3. Verify GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca8beac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# â”€â”€ Pin both Kaggle T4 GPUs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "print(f\"PyTorch version:  {torch.__version__}\")\n",
    "print(f\"CUDA available:   {torch.cuda.is_available()}\")\n",
    "print(f\"GPUs available:   {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        free, total = torch.cuda.mem_get_info(i)\n",
    "        print(f\"\\n  GPU {i}: {props.name}\")\n",
    "        print(f\"    Total VRAM : {total / 1e9:.2f} GB\")\n",
    "        print(f\"    Free  VRAM : {free  / 1e9:.2f} GB\")\n",
    "\n",
    "    total_vram = sum(torch.cuda.get_device_properties(i).total_memory\n",
    "                     for i in range(torch.cuda.device_count()))\n",
    "    print(f\"\\n  Combined VRAM : {total_vram / 1e9:.2f} GB\")\n",
    "    print(\"\\nHuggingFace Trainer will use ALL GPUs via DataParallel automatically.\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected! Training will be extremely slow.\")\n",
    "    print(\"  Kaggle: Settings -> Accelerator -> GPU T4 x2\")\n",
    "    print(\"  Colab:  Runtime -> Change runtime type -> T4 GPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73073b53",
   "metadata": {},
   "source": [
    "## 4. Load SAMSum Dataset\n",
    "\n",
    "Using the **public** dataset `koushik7198/Samsung-samsum_processed` â€” no token required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9d0e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "BASE = \"hf://datasets/koushik7198/Samsung-samsum_processed/\"\n",
    "SPLITS = {\n",
    "    \"train\":      \"data/train-00000-of-00001.parquet\",\n",
    "    \"validation\": \"data/validation-00000-of-00001.parquet\",\n",
    "    \"test\":       \"data/test-00000-of-00001.parquet\",\n",
    "}\n",
    "\n",
    "dataset = DatasetDict()\n",
    "for split, path in SPLITS.items():\n",
    "    df = pd.read_parquet(BASE + path)\n",
    "    # Rename columns: input â†’ dialogue, output â†’ summary\n",
    "    df = df.rename(columns={\"input\": \"dialogue\", \"output\": \"summary\"})\n",
    "    dataset[split] = Dataset.from_pandas(df, preserve_index=False)\n",
    "\n",
    "print(f\"Train:      {len(dataset['train'])} examples\")\n",
    "print(f\"Validation: {len(dataset['validation'])} examples\")\n",
    "print(f\"Test:       {len(dataset['test'])} examples\")\n",
    "print(f\"\\nColumns: {dataset['train'].column_names}\")\n",
    "print(f\"\\n--- Example ---\")\n",
    "print(f\"Dialogue: {dataset['train'][0]['dialogue'][:200]}...\")\n",
    "print(f\"Summary:  {dataset['train'][0]['summary']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e44c70d",
   "metadata": {},
   "source": [
    "## 5. Load Tokenizer & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c03f3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "MODEL_NAME = \"facebook/bart-large-cnn\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Enable gradient checkpointing for memory efficiency\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Model parameters: {model.num_parameters() / 1e6:.0f}M\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5816b178",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# â”€â”€ Model Optimizations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# NOTE: Do NOT manually DataParallel-wrap here â€” HuggingFace Trainer handles\n",
    "#       multi-GPU distribution internally. Manual wrapping causes double-wrap errors.\n",
    "\n",
    "# 1. Move model to primary GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"Model loaded on : {next(model.parameters()).device}\")\n",
    "print(f\"GPUs available  : {torch.cuda.device_count()}\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Trainer will distribute across all {torch.cuda.device_count()} GPUs via DataParallel automatically.\")\n",
    "\n",
    "# 2. Set model to training mode\n",
    "model.train()\n",
    "print(f\"Model mode      : training\")\n",
    "\n",
    "# 3. Mixed precision sanity check\n",
    "if torch.cuda.is_available():\n",
    "    major, minor = torch.cuda.get_device_capability(0)\n",
    "    fp16_ok = major >= 7          # Volta and newer support FP16 natively\n",
    "    print(f\"\\nGPU compute capability : {major}.{minor}  â†’  FP16 {'âœ“ supported' if fp16_ok else 'âœ— not recommended'}\")\n",
    "\n",
    "# 4. Show per-GPU memory footprint after model load\n",
    "if torch.cuda.is_available():\n",
    "    print()\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        allocated = torch.cuda.memory_allocated(i) / 1e9\n",
    "        reserved  = torch.cuda.memory_reserved(i)  / 1e9\n",
    "        free, total = torch.cuda.mem_get_info(i)\n",
    "        print(f\"GPU {i} memory after model load:\")\n",
    "        print(f\"  Allocated : {allocated:.2f} GB\")\n",
    "        print(f\"  Reserved  : {reserved:.2f} GB\")\n",
    "        print(f\"  Free      : {free/1e9:.2f} GB / {total/1e9:.2f} GB\")\n",
    "        print(f\"  Available : {(free/total*100):.1f}% free\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec4a339",
   "metadata": {},
   "source": [
    "## 6. Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e97d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_INPUT_LENGTH = 1024\n",
    "MAX_TARGET_LENGTH = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Tokenize dialogues and summaries.\"\"\"\n",
    "    inputs = examples[\"dialogue\"]\n",
    "    targets = examples[\"summary\"]\n",
    "    \n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    labels = tokenizer(\n",
    "        text_target=targets,\n",
    "        max_length=MAX_TARGET_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    desc=\"Tokenizing\"\n",
    ")\n",
    "\n",
    "print(f\"Tokenized train: {len(tokenized_dataset['train'])}\")\n",
    "print(f\"Tokenized validation: {len(tokenized_dataset['validation'])}\")\n",
    "print(f\"Tokenized test: {len(tokenized_dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab99a63",
   "metadata": {},
   "source": [
    "## 7. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970d55c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# â”€â”€ OOM + disk space fixes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "NUM_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "print(f\"Training on {NUM_GPUS} GPU(s)\")\n",
    "\n",
    "# â”€â”€ Disk space check before training â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "total, used, free = shutil.disk_usage(\"/kaggle/working\")\n",
    "print(f\"Disk before training: {free/1e9:.1f} GB free / {total/1e9:.1f} GB total\")\n",
    "if free / 1e9 < 5:\n",
    "    print(\"WARNING: Less than 5 GB free! Training may crash with 'No space left on device'.\")\n",
    "    print(\"  â†’ Delete unused files or reduce save_total_limit further.\")\n",
    "\n",
    "# ROUGE metric â€” only used at test time, NOT during training eval\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute ROUGE â€” called only during final test evaluation, not mid-training.\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    return {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "\n",
    "OUTPUT_DIR = \"./bart-samsum-finetuned\"\n",
    "\n",
    "PER_DEVICE_TRAIN_BS = 2\n",
    "PER_DEVICE_EVAL_BS  = 1\n",
    "GRAD_ACCUM          = 8\n",
    "\n",
    "# ~847 total steps â†’ 10% warmup\n",
    "ESTIMATED_TOTAL_STEPS = 850\n",
    "WARMUP_STEPS = max(50, ESTIMATED_TOTAL_STEPS // 10)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=PER_DEVICE_TRAIN_BS,\n",
    "    per_device_eval_batch_size=PER_DEVICE_EVAL_BS,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "\n",
    "    # Evaluation\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "\n",
    "    # â”€â”€ DISK FIX: only keep 1 checkpoint at a time â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Each checkpoint = ~1.6 GB. With 5 checkpoints = 8 GB â†’ disk full crash.\n",
    "    # Keep only the single best checkpoint to stay within 20 GB Kaggle limit.\n",
    "    save_total_limit=1,\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "\n",
    "    # No logit accumulation during training evals (OOM fix)\n",
    "    prediction_loss_only=True,\n",
    "\n",
    "    # Performance\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    gradient_checkpointing=True,\n",
    "    dataloader_num_workers=0,   # avoids Kaggle fork/deadlock crash\n",
    "\n",
    "    # Logging\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "eff_batch = PER_DEVICE_TRAIN_BS * NUM_GPUS * GRAD_ACCUM\n",
    "print(\"\\nTraining configuration:\")\n",
    "print(f\"  GPUs:                     {NUM_GPUS}\")\n",
    "print(f\"  Effective batch size:     {eff_batch}  ({PER_DEVICE_TRAIN_BS} Ã— {NUM_GPUS} GPUs Ã— {GRAD_ACCUM})\")\n",
    "print(f\"  Learning rate:            {training_args.learning_rate}\")\n",
    "print(f\"  Warmup steps:             {WARMUP_STEPS}  (~10% of ~{ESTIMATED_TOTAL_STEPS})\")\n",
    "print(f\"  FP16:                     {training_args.fp16}\")\n",
    "print(f\"  Eval/Save every:          {training_args.eval_steps} steps\")\n",
    "print(f\"  save_total_limit:         1  â† only 1 checkpoint kept (~1.6 GB max)\")\n",
    "print(f\"  prediction_loss_only:     True  â† no logit gather during eval\")\n",
    "print(f\"  dataloader_num_workers:   0  â† no forked workers\")\n",
    "print(f\"  PYTORCH_CUDA_ALLOC_CONF:  {os.environ['PYTORCH_CUDA_ALLOC_CONF']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf68613",
   "metadata": {},
   "source": [
    "## 8. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871be3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import torch\n",
    "\n",
    "# â”€â”€ Check if the model was already saved from a previous run â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "FINAL_SAVE_DIR = \"./bart-samsum-final\"\n",
    "prior_save_exists = os.path.exists(os.path.join(FINAL_SAVE_DIR, \"config.json\"))\n",
    "if prior_save_exists:\n",
    "    size = sum(os.path.getsize(os.path.join(FINAL_SAVE_DIR, f))\n",
    "               for f in os.listdir(FINAL_SAVE_DIR)) / 1e9\n",
    "    print(f\"Found existing saved model at {FINAL_SAVE_DIR} ({size:.2f} GB)\")\n",
    "    print(\"Skip to cell 11 (Save) to re-save, or cell 10 (Inference) to test it.\")\n",
    "    print(\"Set FORCE_RETRAIN = True below to retrain anyway.\")\n",
    "    FORCE_RETRAIN = False\n",
    "else:\n",
    "    FORCE_RETRAIN = True\n",
    "\n",
    "if FORCE_RETRAIN or not prior_save_exists:\n",
    "    # â”€â”€ Show per-GPU free memory before training â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            free, total = torch.cuda.mem_get_info(i)\n",
    "            print(f\"GPU {i} free: {free/1e9:.2f} GB / {total/1e9:.2f} GB\")\n",
    "\n",
    "    disk_free = shutil.disk_usage(\"/kaggle/working\").free / 1e9\n",
    "    print(f\"Disk free before training: {disk_free:.1f} GB\")\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"validation\"],\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "    )\n",
    "\n",
    "    # â”€â”€ Auto-resume from latest checkpoint â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    search_paths = [\n",
    "        training_args.output_dir,\n",
    "        os.path.join(\"/kaggle/working\", \"bart-samsum-finetuned\"),\n",
    "        os.path.join(\"/kaggle/working\", \"Text-Summarization-using-NLP\", \"bart-samsum-finetuned\"),\n",
    "    ]\n",
    "    resume_ckpt = None\n",
    "    for search_dir in search_paths:\n",
    "        ckpts = sorted(glob.glob(os.path.join(search_dir, \"checkpoint-*\")))\n",
    "        if ckpts:\n",
    "            resume_ckpt = ckpts[-1]\n",
    "            break\n",
    "\n",
    "    if resume_ckpt:\n",
    "        print(f\"\\nResuming from checkpoint: {resume_ckpt}\")\n",
    "    else:\n",
    "        print(\"\\nNo checkpoint found â€” starting from scratch.\")\n",
    "\n",
    "    num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "    print(f\"Starting training on {num_gpus} GPU(s)... (~2 hours on 2Ã— T4)\")\n",
    "    train_result = trainer.train(resume_from_checkpoint=resume_ckpt)\n",
    "\n",
    "    # â”€â”€ Save best model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    trainer.save_model(training_args.output_dir)\n",
    "\n",
    "    disk_free_after = shutil.disk_usage(\"/kaggle/working\").free / 1e9\n",
    "    print(f\"\\nDisk free after training: {disk_free_after:.1f} GB\")\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"Training Complete!\")\n",
    "    print(f\"{'='*50}\")\n",
    "    for k, v in sorted(train_result.metrics.items()):\n",
    "        print(f\"  {k}: {v:.4f}\" if isinstance(v, float) else f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cb1192",
   "metadata": {},
   "source": [
    "## 9. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751d3647",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# â”€â”€ Test set ROUGE evaluation â€” batched generation (no RAM accumulation) â”€â”€â”€â”€â”€â”€\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "# Resolve which model to use:\n",
    "#   1. trainer.model  â€” if training ran this session\n",
    "#   2. fine-tuned weights on disk  â€” if training ran in a previous session\n",
    "#   3. base model  â€” last resort (results will be poor)\n",
    "try:\n",
    "    eval_model = trainer.model\n",
    "    print(\"Using trainer.model (fine-tuned this session)\")\n",
    "except NameError:\n",
    "    for candidate in [\"./bart-samsum-final\", \"./bart-samsum-finetuned\"]:\n",
    "        if os.path.exists(os.path.join(candidate, \"config.json\")):\n",
    "            print(f\"Loading fine-tuned model from {candidate} ...\")\n",
    "            _dev = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "            eval_model = AutoModelForSeq2SeqLM.from_pretrained(candidate).to(_dev)\n",
    "            print(\"Fine-tuned model loaded successfully.\")\n",
    "            break\n",
    "    else:\n",
    "        print(\"WARNING: No fine-tuned model found â€” using base model. Results will be poor.\")\n",
    "        eval_model = model\n",
    "\n",
    "eval_model.eval()\n",
    "device = next(eval_model.parameters()).device\n",
    "print(f\"Eval device: {device}\")\n",
    "\n",
    "EVAL_BATCH_SIZE = 4    # reduce to 2 if this cell still OOMs\n",
    "\n",
    "all_preds, all_refs = [], []\n",
    "\n",
    "# Set format to torch so DataLoader returns tensors directly\n",
    "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    tokenized_dataset[\"test\"],\n",
    "    batch_size=EVAL_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "print(f\"Evaluating {len(tokenized_dataset['test'])} test examples \"\n",
    "      f\"in batches of {EVAL_BATCH_SIZE}...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step, batch in enumerate(test_loader):\n",
    "        input_ids      = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        label_ids      = batch[\"labels\"]   # CPU only\n",
    "\n",
    "        generated = eval_model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=128,\n",
    "            num_beams=1,\n",
    "            no_repeat_ngram_size=5,\n",
    "            early_stopping=True,\n",
    "        )\n",
    "\n",
    "        decoded_preds  = tokenizer.batch_decode(generated.cpu(), skip_special_tokens=True)\n",
    "        lab_np = label_ids.numpy().copy()\n",
    "        lab_np[lab_np == -100] = tokenizer.pad_token_id\n",
    "        decoded_labels = tokenizer.batch_decode(lab_np, skip_special_tokens=True)\n",
    "\n",
    "        all_preds.extend(decoded_preds)\n",
    "        all_refs.extend(decoded_labels)\n",
    "\n",
    "        del input_ids, attention_mask, generated\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if (step + 1) % 50 == 0:\n",
    "            done = min((step + 1) * EVAL_BATCH_SIZE, len(tokenized_dataset[\"test\"]))\n",
    "            print(f\"  [{done}/{len(tokenized_dataset['test'])}] done\")\n",
    "\n",
    "rouge_scores = rouge.compute(\n",
    "    predictions=all_preds,\n",
    "    references=all_refs,\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Test Set ROUGE Scores (fine-tuned BART)\")\n",
    "print(f\"{'='*50}\")\n",
    "for k in [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]:\n",
    "    print(f\"  {k:12s}: {rouge_scores[k]:.4f}\")\n",
    "print(f\"\\n  Evaluated on {len(all_preds)} examples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4620e13f",
   "metadata": {},
   "source": [
    "## 10. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcee4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "# Resolve which model to use (same logic as eval cell)\n",
    "try:\n",
    "    infer_model = trainer.model\n",
    "    print(\"Using trainer.model (fine-tuned this session)\")\n",
    "except NameError:\n",
    "    for candidate in [\"./bart-samsum-final\", \"./bart-samsum-finetuned\"]:\n",
    "        if os.path.exists(os.path.join(candidate, \"config.json\")):\n",
    "            print(f\"Loading fine-tuned model from {candidate} ...\")\n",
    "            _dev = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "            infer_model = AutoModelForSeq2SeqLM.from_pretrained(candidate).to(_dev)\n",
    "            print(\"Fine-tuned model loaded successfully.\")\n",
    "            break\n",
    "    else:\n",
    "        print(\"WARNING: No fine-tuned model found â€” using base model. Results will be poor.\")\n",
    "        infer_model = model\n",
    "\n",
    "infer_model.eval()\n",
    "infer_device = next(infer_model.parameters()).device\n",
    "\n",
    "def summarize(text, max_new_tokens=60, num_beams=1):\n",
    "    \"\"\"Generate a summary using model.generate() directly (pipeline-free).\"\"\"\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=1024,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "    )\n",
    "    input_ids      = inputs[\"input_ids\"].to(infer_device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(infer_device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = infer_model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=5,   # prevents repetitive / hallucinated phrases\n",
    "            length_penalty=0.8,       # encourages shorter, tighter summaries\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "test_dialogues = [\n",
    "    \"\"\"Amanda: Hey, are we meeting today?\n",
    "Jerry: Sure! What time works for you?\n",
    "Amanda: How about 3pm at the coffee shop?\n",
    "Jerry: Perfect, see you there!\n",
    "Amanda: Great, I'll bring the project reports.\"\"\",\n",
    "\n",
    "    \"\"\"Sarah: Did you finish the report?\n",
    "Mike: Almost done, just need to review the numbers.\n",
    "Sarah: The deadline is tomorrow at noon.\n",
    "Mike: I know, I'll send it tonight.\n",
    "Sarah: Great, also include the Q3 projections.\n",
    "Mike: Will do. Should I CC the manager?\n",
    "Sarah: Yes, please CC David.\"\"\",\n",
    "\n",
    "    \"\"\"Tom: Happy birthday! ðŸŽ‚\n",
    "Lisa: Thank you so much! ðŸ˜Š\n",
    "Tom: Are you doing anything special?\n",
    "Lisa: Just a small dinner with family.\n",
    "Tom: That sounds lovely. Enjoy your day!\"\"\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INFERENCE EXAMPLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, dialogue in enumerate(test_dialogues, 1):\n",
    "    summary = summarize(dialogue)\n",
    "    print(f\"\\n--- Example {i} ---\")\n",
    "    print(f\"Dialogue:\\n{dialogue}\")\n",
    "    print(f\"\\nSummary: {summary}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6070ee9c",
   "metadata": {},
   "source": [
    "## 11. Save & Download Model\n",
    "\n",
    "Save the fine-tuned model. On Kaggle, download from the **Output** tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420f970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "SAVE_DIR = \"./bart-samsum-final\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Use trainer.model if available, otherwise load from the saved checkpoint dir\n",
    "try:\n",
    "    save_model = trainer.model\n",
    "    save_model.save_pretrained(SAVE_DIR)\n",
    "    tokenizer.save_pretrained(SAVE_DIR)\n",
    "    print(f\"Model saved from trainer â†’ {SAVE_DIR}\")\n",
    "except NameError:\n",
    "    # trainer was not run (model loaded from prior run) â€” copy from OUTPUT_DIR\n",
    "    OUTPUT_DIR = \"./bart-samsum-finetuned\"\n",
    "    if os.path.exists(OUTPUT_DIR):\n",
    "        for f in os.listdir(OUTPUT_DIR):\n",
    "            if not f.startswith(\"checkpoint\"):\n",
    "                shutil.copy2(os.path.join(OUTPUT_DIR, f), SAVE_DIR)\n",
    "        tokenizer.save_pretrained(SAVE_DIR)\n",
    "        print(f\"Model copied from {OUTPUT_DIR} â†’ {SAVE_DIR}\")\n",
    "    else:\n",
    "        print(\"ERROR: No trained model found. Run the training cell first.\")\n",
    "\n",
    "# Show saved files and total size\n",
    "total_size = 0\n",
    "for f in sorted(os.listdir(SAVE_DIR)):\n",
    "    size = os.path.getsize(os.path.join(SAVE_DIR, f))\n",
    "    total_size += size\n",
    "    print(f\"  {f}: {size / 1e6:.1f} MB\")\n",
    "\n",
    "print(f\"\\nTotal model size: {total_size / 1e9:.2f} GB\")\n",
    "\n",
    "# Final disk status\n",
    "free = shutil.disk_usage(\"/kaggle/working\").free / 1e9\n",
    "print(f\"Disk remaining:   {free:.1f} GB free\")\n",
    "print(\"\\nOn Kaggle: Go to the Output tab â†’ download bart-samsum-final/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e01a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# â”€â”€ Zip the model for easy single-file download â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "SAVE_DIR  = \"./bart-samsum-final\"\n",
    "ZIP_PATH  = \"/kaggle/working/bart-samsum-final.zip\"\n",
    "\n",
    "if not os.path.exists(SAVE_DIR):\n",
    "    print(f\"ERROR: {SAVE_DIR} not found â€” run the Save cell first.\")\n",
    "else:\n",
    "    print(f\"Zipping {SAVE_DIR} â†’ {ZIP_PATH} ...\")\n",
    "    with zipfile.ZipFile(ZIP_PATH, \"w\", zipfile.ZIP_DEFLATED) as zf:\n",
    "        for fname in sorted(os.listdir(SAVE_DIR)):\n",
    "            fpath = os.path.join(SAVE_DIR, fname)\n",
    "            zf.write(fpath, arcname=os.path.join(\"bart-samsum-final\", fname))\n",
    "            print(f\"  added: {fname}  ({os.path.getsize(fpath)/1e6:.1f} MB)\")\n",
    "\n",
    "    zip_size = os.path.getsize(ZIP_PATH) / 1e9\n",
    "    free     = shutil.disk_usage(\"/kaggle/working\").free / 1e9\n",
    "    print(f\"\\nZip created : {ZIP_PATH}\")\n",
    "    print(f\"Zip size    : {zip_size:.2f} GB\")\n",
    "    print(f\"Disk free   : {free:.1f} GB\")\n",
    "    print(\"\\nTo download on Kaggle:\")\n",
    "    print(\"  Output tab (right panel) â†’ find bart-samsum-final.zip â†’ click Download\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621f76a6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## After Training\n",
    "\n",
    "1. **Download** the `bart-samsum-final/` folder\n",
    "2. **Copy** it to your project at `artifacts/model_trainer/bart-samsum-model/`\n",
    "3. **Run** the API server: `python app.py`\n",
    "4. **Open** `http://localhost:8080` for the web UI\n",
    "\n",
    "### Expected Results (3 epochs)\n",
    "\n",
    "| Metric | Score |\n",
    "|--------|-------|\n",
    "| ROUGE-1 | ~0.52 |\n",
    "| ROUGE-2 | ~0.28 |\n",
    "| ROUGE-L | ~0.43 |\n",
    "| ROUGE-Lsum | ~0.48 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
